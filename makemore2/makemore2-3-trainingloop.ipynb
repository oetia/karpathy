{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open(\"data/names.txt\").read()\n",
    "names = raw.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(names) + \".\")))\n",
    "char_to_num = { char: num for num, char in enumerate(chars) }\n",
    "num_to_char = { num: char for char, num in char_to_num.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 2, 27]), torch.Size([228146, 27]), torch.Size([228146]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []; y_idx = []\n",
    "eye = torch.eye(len(chars))\n",
    "\n",
    "context_window = 2\n",
    "for name in names:\n",
    "  string = \".\" * (context_window) + name + \".\"\n",
    "  string_nums = [char_to_num[char] for char in string]\n",
    "\n",
    "  for idx in range(context_window, len(string)):\n",
    "    substring_nums = string_nums[idx - context_window:idx]\n",
    "    target_num = string_nums[idx]\n",
    "    x = eye[substring_nums]; X.append(x)\n",
    "    y = eye[target_num]; Y.append(y); y_idx.append(target_num)\n",
    "    \n",
    "    # print(f\"{string[idx - context_window:idx]} -> {string[idx]}\")\n",
    "    # print(f\"{substring_nums} -> {target_num}\")\n",
    "    # print(x, y)\n",
    "\n",
    "X = torch.stack(X) # stack to merge list of tensors\n",
    "Y = torch.stack(Y); y_idx = torch.tensor(y_idx)\n",
    "\n",
    "X.shape, Y.shape, y_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing parameters\n",
    "generator = torch.Generator().manual_seed(14)\n",
    "embedding_vector_dimensionality = 2\n",
    "embedding_matrix = torch.randn((len(chars), embedding_vector_dimensionality), generator=generator, requires_grad=True)\n",
    "\n",
    "hidden_layer_num_neurons = 100\n",
    "hidden_layer_weights = torch.randn((context_window * embedding_vector_dimensionality, hidden_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "hidden_layer_biases = torch.randn((hidden_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "\n",
    "output_layer_num_neurons = len(chars)\n",
    "output_layer_weights = torch.randn((hidden_layer_num_neurons, output_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "output_layer_biases = torch.randn((output_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "\n",
    "parameters = [embedding_matrix, hidden_layer_weights, hidden_layer_biases, output_layer_weights, output_layer_biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 2, 2])\n",
      "torch.Size([228146, 4])\n",
      "torch.Size([228146, 100])\n",
      "torch.Size([228146, 27])\n",
      "torch.Size([228146, 27]) tensor(True)\n",
      "torch.Size([228146])\n",
      "tensor(15.7477, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "embeddings = X @ embedding_matrix\n",
    "print(embeddings.shape)\n",
    "\n",
    "embeddings_flattened = embeddings.view(-1, context_window * embedding_vector_dimensionality) # think about traversal order\n",
    "print(embeddings_flattened.shape)\n",
    "\n",
    "hidden_layer_preactivations = embeddings_flattened @ hidden_layer_weights + hidden_layer_biases\n",
    "# hidden_layer_activations = torch.maximum(hidden_layer_preactivations, torch.tensor(0.0))\n",
    "hidden_layer_activations = hidden_layer_preactivations.tanh()\n",
    "print(hidden_layer_activations.shape)\n",
    "\n",
    "output_layer_preactivations = hidden_layer_activations @ output_layer_weights + output_layer_biases\n",
    "output_layer_activations = output_layer_preactivations\n",
    "logits = output_layer_activations\n",
    "print(logits.shape)\n",
    "\n",
    "logits_sub_max = logits - logits.max(dim=1, keepdim=True).values\n",
    "counts = logits_sub_max.exp()\n",
    "prob_distributions = counts / counts.sum(dim=1, keepdim=True)\n",
    "print(prob_distributions.shape, prob_distributions.sum(dim=1).isclose(torch.tensor(1.0)).all())\n",
    "\n",
    "target_probs = prob_distributions[torch.arange(X.shape[0]), y_idx]\n",
    "target_logprobs = target_probs.log()\n",
    "print(target_logprobs.shape)\n",
    "\n",
    "negative_average_log_likelihood = -target_logprobs.mean()\n",
    "loss = negative_average_log_likelihood\n",
    "print(loss)\n",
    "\n",
    "intermediates = [embeddings, embeddings_flattened, hidden_layer_preactivations, hidden_layer_activations, output_layer_preactivations, output_layer_activations, logits, logits_sub_max, counts, prob_distributions, target_probs, target_logprobs]\n",
    "params_and_intermediates = parameters + intermediates\n",
    "\n",
    "# tensor(15.7477, grad_fn=<NegBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "for tensor in params_and_intermediates:\n",
    "  tensor.grad = None\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss.backward()\n",
    "embedding_matrix.data = embedding_matrix.data - learning_rate * embedding_matrix.grad\n",
    "hidden_layer_weights.data = hidden_layer_weights.data - learning_rate * hidden_layer_weights.grad\n",
    "hidden_layer_biases.data = hidden_layer_biases.data - learning_rate * hidden_layer_biases.grad\n",
    "output_layer_weights.data = output_layer_weights.data - learning_rate * output_layer_weights.grad\n",
    "output_layer_biases.data = output_layer_biases.data - learning_rate * output_layer_biases.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation graph gets nuked after backward() called. don't need to worry about duplicate nodes getting created each forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(3.1470, grad_fn=<NegBackward0>)\n",
      "1 tensor(3.1405, grad_fn=<NegBackward0>)\n",
      "2 tensor(3.1341, grad_fn=<NegBackward0>)\n",
      "3 tensor(3.1278, grad_fn=<NegBackward0>)\n",
      "4 tensor(3.1217, grad_fn=<NegBackward0>)\n",
      "5 tensor(3.1156, grad_fn=<NegBackward0>)\n",
      "6 tensor(3.1098, grad_fn=<NegBackward0>)\n",
      "7 tensor(3.1040, grad_fn=<NegBackward0>)\n",
      "8 tensor(3.0984, grad_fn=<NegBackward0>)\n",
      "9 tensor(3.0929, grad_fn=<NegBackward0>)\n",
      "10 tensor(3.0875, grad_fn=<NegBackward0>)\n",
      "11 tensor(3.0822, grad_fn=<NegBackward0>)\n",
      "12 tensor(3.0770, grad_fn=<NegBackward0>)\n",
      "13 tensor(3.0720, grad_fn=<NegBackward0>)\n",
      "14 tensor(3.0671, grad_fn=<NegBackward0>)\n",
      "15 tensor(3.0623, grad_fn=<NegBackward0>)\n",
      "16 tensor(3.0576, grad_fn=<NegBackward0>)\n",
      "17 tensor(3.0530, grad_fn=<NegBackward0>)\n",
      "18 tensor(3.0485, grad_fn=<NegBackward0>)\n",
      "19 tensor(3.0441, grad_fn=<NegBackward0>)\n",
      "20 tensor(3.0399, grad_fn=<NegBackward0>)\n",
      "21 tensor(3.0357, grad_fn=<NegBackward0>)\n",
      "22 tensor(3.0317, grad_fn=<NegBackward0>)\n",
      "23 tensor(3.0277, grad_fn=<NegBackward0>)\n",
      "24 tensor(3.0238, grad_fn=<NegBackward0>)\n",
      "25 tensor(3.0200, grad_fn=<NegBackward0>)\n",
      "26 tensor(3.0163, grad_fn=<NegBackward0>)\n",
      "27 tensor(3.0127, grad_fn=<NegBackward0>)\n",
      "28 tensor(3.0091, grad_fn=<NegBackward0>)\n",
      "29 tensor(3.0057, grad_fn=<NegBackward0>)\n",
      "30 tensor(3.0023, grad_fn=<NegBackward0>)\n",
      "31 tensor(2.9989, grad_fn=<NegBackward0>)\n",
      "32 tensor(2.9956, grad_fn=<NegBackward0>)\n",
      "33 tensor(2.9924, grad_fn=<NegBackward0>)\n",
      "34 tensor(2.9892, grad_fn=<NegBackward0>)\n",
      "35 tensor(2.9861, grad_fn=<NegBackward0>)\n",
      "36 tensor(2.9830, grad_fn=<NegBackward0>)\n",
      "37 tensor(2.9800, grad_fn=<NegBackward0>)\n",
      "38 tensor(2.9770, grad_fn=<NegBackward0>)\n",
      "39 tensor(2.9741, grad_fn=<NegBackward0>)\n",
      "40 tensor(2.9712, grad_fn=<NegBackward0>)\n",
      "41 tensor(2.9684, grad_fn=<NegBackward0>)\n",
      "42 tensor(2.9656, grad_fn=<NegBackward0>)\n",
      "43 tensor(2.9628, grad_fn=<NegBackward0>)\n",
      "44 tensor(2.9601, grad_fn=<NegBackward0>)\n",
      "45 tensor(2.9574, grad_fn=<NegBackward0>)\n",
      "46 tensor(2.9547, grad_fn=<NegBackward0>)\n",
      "47 tensor(2.9521, grad_fn=<NegBackward0>)\n",
      "48 tensor(2.9495, grad_fn=<NegBackward0>)\n",
      "49 tensor(2.9470, grad_fn=<NegBackward0>)\n",
      "50 tensor(2.9445, grad_fn=<NegBackward0>)\n",
      "51 tensor(2.9420, grad_fn=<NegBackward0>)\n",
      "52 tensor(2.9395, grad_fn=<NegBackward0>)\n",
      "53 tensor(2.9371, grad_fn=<NegBackward0>)\n",
      "54 tensor(2.9347, grad_fn=<NegBackward0>)\n",
      "55 tensor(2.9323, grad_fn=<NegBackward0>)\n",
      "56 tensor(2.9300, grad_fn=<NegBackward0>)\n",
      "57 tensor(2.9277, grad_fn=<NegBackward0>)\n",
      "58 tensor(2.9254, grad_fn=<NegBackward0>)\n",
      "59 tensor(2.9231, grad_fn=<NegBackward0>)\n",
      "60 tensor(2.9209, grad_fn=<NegBackward0>)\n",
      "61 tensor(2.9187, grad_fn=<NegBackward0>)\n",
      "62 tensor(2.9165, grad_fn=<NegBackward0>)\n",
      "63 tensor(2.9143, grad_fn=<NegBackward0>)\n",
      "64 tensor(2.9122, grad_fn=<NegBackward0>)\n",
      "65 tensor(2.9100, grad_fn=<NegBackward0>)\n",
      "66 tensor(2.9079, grad_fn=<NegBackward0>)\n",
      "67 tensor(2.9059, grad_fn=<NegBackward0>)\n",
      "68 tensor(2.9038, grad_fn=<NegBackward0>)\n",
      "69 tensor(2.9018, grad_fn=<NegBackward0>)\n",
      "70 tensor(2.8997, grad_fn=<NegBackward0>)\n",
      "71 tensor(2.8977, grad_fn=<NegBackward0>)\n",
      "72 tensor(2.8957, grad_fn=<NegBackward0>)\n",
      "73 tensor(2.8938, grad_fn=<NegBackward0>)\n",
      "74 tensor(2.8918, grad_fn=<NegBackward0>)\n",
      "75 tensor(2.8899, grad_fn=<NegBackward0>)\n",
      "76 tensor(2.8880, grad_fn=<NegBackward0>)\n",
      "77 tensor(2.8861, grad_fn=<NegBackward0>)\n",
      "78 tensor(2.8842, grad_fn=<NegBackward0>)\n",
      "79 tensor(2.8823, grad_fn=<NegBackward0>)\n",
      "80 tensor(2.8805, grad_fn=<NegBackward0>)\n",
      "81 tensor(2.8787, grad_fn=<NegBackward0>)\n",
      "82 tensor(2.8769, grad_fn=<NegBackward0>)\n",
      "83 tensor(2.8751, grad_fn=<NegBackward0>)\n",
      "84 tensor(2.8733, grad_fn=<NegBackward0>)\n",
      "85 tensor(2.8715, grad_fn=<NegBackward0>)\n",
      "86 tensor(2.8698, grad_fn=<NegBackward0>)\n",
      "87 tensor(2.8680, grad_fn=<NegBackward0>)\n",
      "88 tensor(2.8663, grad_fn=<NegBackward0>)\n",
      "89 tensor(2.8646, grad_fn=<NegBackward0>)\n",
      "90 tensor(2.8629, grad_fn=<NegBackward0>)\n",
      "91 tensor(2.8613, grad_fn=<NegBackward0>)\n",
      "92 tensor(2.8596, grad_fn=<NegBackward0>)\n",
      "93 tensor(2.8580, grad_fn=<NegBackward0>)\n",
      "94 tensor(2.8564, grad_fn=<NegBackward0>)\n",
      "95 tensor(2.8548, grad_fn=<NegBackward0>)\n",
      "96 tensor(2.8532, grad_fn=<NegBackward0>)\n",
      "97 tensor(2.8516, grad_fn=<NegBackward0>)\n",
      "98 tensor(2.8500, grad_fn=<NegBackward0>)\n",
      "99 tensor(2.8485, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "\n",
    "  # forward\n",
    "  embeddings = X @ embedding_matrix\n",
    "  embeddings_flattened = embeddings.view(-1, context_window * embedding_vector_dimensionality) # think about traversal order\n",
    "\n",
    "  hidden_layer_preactivations = embeddings_flattened @ hidden_layer_weights + hidden_layer_biases\n",
    "  hidden_layer_activations = hidden_layer_preactivations.tanh()\n",
    "\n",
    "  output_layer_preactivations = hidden_layer_activations @ output_layer_weights + output_layer_biases\n",
    "  output_layer_activations = output_layer_preactivations\n",
    "  logits = output_layer_activations\n",
    "\n",
    "  logits_sub_max = logits - logits.max(dim=1, keepdim=True).values\n",
    "  counts = logits_sub_max.exp()\n",
    "  prob_distributions = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "  target_probs = prob_distributions[torch.arange(X.shape[0]), y_idx]\n",
    "  target_logprobs = target_probs.log()\n",
    "  negative_average_log_likelihood = -target_logprobs.mean()\n",
    "  loss = negative_average_log_likelihood\n",
    "\n",
    "  losses.append(loss); print(iter, loss)\n",
    "\n",
    "  intermediates = [embeddings, embeddings_flattened, hidden_layer_preactivations, hidden_layer_activations, output_layer_preactivations, output_layer_activations, logits, logits_sub_max, counts, prob_distributions, target_probs, target_logprobs] # new objects created each forward pass, so i think i need to redefine this each time\n",
    "  params_and_intermediates = parameters + intermediates\n",
    "\n",
    "  # backward\n",
    "  for tensor in params_and_intermediates:\n",
    "    tensor.grad = None\n",
    "\n",
    "  learning_rate = 0.1\n",
    "  loss.backward()\n",
    "  embedding_matrix.data = embedding_matrix.data - learning_rate * embedding_matrix.grad\n",
    "  hidden_layer_weights.data = hidden_layer_weights.data - learning_rate * hidden_layer_weights.grad\n",
    "  hidden_layer_biases.data = hidden_layer_biases.data - learning_rate * hidden_layer_biases.grad\n",
    "  output_layer_weights.data = output_layer_weights.data - learning_rate * output_layer_weights.grad\n",
    "  output_layer_biases.data = output_layer_biases.data - learning_rate * output_layer_biases.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
