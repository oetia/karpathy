{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open(\"data/names.txt\").read()\n",
    "names = raw.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(names) + \".\")))\n",
    "char_to_num = { char: num for num, char in enumerate(chars) }\n",
    "num_to_char = { num: char for char, num in char_to_num.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3, 27]), torch.Size([228146, 27]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []; y_idx = []\n",
    "eye = torch.eye(len(chars))\n",
    "\n",
    "context_window = 4\n",
    "for name in names:\n",
    "  string = \".\" * (context_window) + name + \".\"\n",
    "  string_nums = [char_to_num[char] for char in string]\n",
    "\n",
    "  for idx in range(context_window, len(string)):\n",
    "    substring_nums = string_nums[idx - context_window:idx]\n",
    "    target_num = string_nums[idx]\n",
    "    x = eye[substring_nums]; X.append(x)\n",
    "    y = eye[target_num]; Y.append(y); y_idx.append(target_num)\n",
    "    \n",
    "    # print(f\"{string[idx - context_window:idx]} -> {string[idx]}\")\n",
    "    # print(f\"{substring_nums} -> {target_num}\")\n",
    "    # print(x, y)\n",
    "\n",
    "X = torch.stack(X) # stack to merge list of tensors\n",
    "Y = torch.stack(Y); y_idx = torch.tensor(y_idx)\n",
    "\n",
    "X.shape, Y.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing parameters\n",
    "generator = torch.Generator().manual_seed(14)\n",
    "embedding_vector_dimensionality = 2\n",
    "embedding_matrix = torch.randn((len(chars), embedding_vector_dimensionality), generator=generator, requires_grad=True)\n",
    "\n",
    "hidden_layer_num_neurons = 200\n",
    "hidden_layer_weights = torch.randn((context_window * embedding_vector_dimensionality, hidden_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "hidden_layer_biases = torch.randn((hidden_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "\n",
    "output_layer_num_neurons = len(chars)\n",
    "output_layer_weights = torch.randn((hidden_layer_num_neurons, output_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "output_layer_biases = torch.randn((output_layer_num_neurons), generator=generator, requires_grad=True)\n",
    "\n",
    "parameters = [embedding_matrix, hidden_layer_weights, hidden_layer_biases, output_layer_weights, output_layer_biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3, 2])\n",
      "torch.Size([228146, 6])\n",
      "torch.Size([228146, 100])\n",
      "torch.Size([228146, 27])\n",
      "torch.Size([228146, 27]) tensor(True)\n",
      "torch.Size([228146])\n",
      "tensor(18.2370, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "embeddings = X @ embedding_matrix\n",
    "print(embeddings.shape)\n",
    "\n",
    "embeddings_flattened = embeddings.view(-1, context_window * embedding_vector_dimensionality) # think about traversal order\n",
    "print(embeddings_flattened.shape)\n",
    "\n",
    "hidden_layer_preactivations = embeddings_flattened @ hidden_layer_weights + hidden_layer_biases\n",
    "# hidden_layer_activations = torch.maximum(hidden_layer_preactivations, torch.tensor(0.0))\n",
    "hidden_layer_activations = hidden_layer_preactivations.tanh()\n",
    "print(hidden_layer_activations.shape)\n",
    "\n",
    "output_layer_preactivations = hidden_layer_activations @ output_layer_weights + output_layer_biases\n",
    "output_layer_activations = output_layer_preactivations\n",
    "logits = output_layer_activations\n",
    "print(logits.shape)\n",
    "\n",
    "logits_sub_max = logits - logits.max(dim=1, keepdim=True).values\n",
    "counts = logits_sub_max.exp()\n",
    "prob_distributions = counts / counts.sum(dim=1, keepdim=True)\n",
    "print(prob_distributions.shape, prob_distributions.sum(dim=1).isclose(torch.tensor(1.0)).all())\n",
    "\n",
    "target_probs = prob_distributions[torch.arange(X.shape[0]), y_idx]\n",
    "target_logprobs = target_probs.log()\n",
    "print(target_logprobs.shape)\n",
    "\n",
    "negative_average_log_likelihood = -target_logprobs.mean()\n",
    "loss = negative_average_log_likelihood\n",
    "print(loss)\n",
    "\n",
    "intermediates = [embeddings, embeddings_flattened, hidden_layer_preactivations, hidden_layer_activations, output_layer_preactivations, output_layer_activations, logits, logits_sub_max, counts, prob_distributions, target_probs, target_logprobs]\n",
    "params_and_intermediates = parameters + intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "for tensor in params_and_intermediates:\n",
    "  tensor.grad = None\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss.backward()\n",
    "embedding_matrix.data = embedding_matrix.data - learning_rate * embedding_matrix.grad\n",
    "hidden_layer_weights.data = hidden_layer_weights.data - learning_rate * hidden_layer_weights.grad\n",
    "hidden_layer_biases.data = hidden_layer_biases.data - learning_rate * hidden_layer_biases.grad\n",
    "output_layer_weights.data = output_layer_weights.data - learning_rate * output_layer_weights.grad\n",
    "output_layer_biases.data = output_layer_biases.data - learning_rate * output_layer_biases.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation graph gets nuked after backward() called. don't need to worry about duplicate nodes getting created each forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(3.5407, grad_fn=<NegBackward0>)\n",
      "1 tensor(3.4009, grad_fn=<NegBackward0>)\n",
      "2 tensor(3.3235, grad_fn=<NegBackward0>)\n",
      "3 tensor(3.2734, grad_fn=<NegBackward0>)\n",
      "4 tensor(3.2395, grad_fn=<NegBackward0>)\n",
      "5 tensor(3.2163, grad_fn=<NegBackward0>)\n",
      "6 tensor(3.2004, grad_fn=<NegBackward0>)\n",
      "7 tensor(3.1896, grad_fn=<NegBackward0>)\n",
      "8 tensor(3.1822, grad_fn=<NegBackward0>)\n",
      "9 tensor(3.1770, grad_fn=<NegBackward0>)\n",
      "10 tensor(3.1732, grad_fn=<NegBackward0>)\n",
      "11 tensor(3.1704, grad_fn=<NegBackward0>)\n",
      "12 tensor(3.1680, grad_fn=<NegBackward0>)\n",
      "13 tensor(3.1660, grad_fn=<NegBackward0>)\n",
      "14 tensor(3.1642, grad_fn=<NegBackward0>)\n",
      "15 tensor(3.1625, grad_fn=<NegBackward0>)\n",
      "16 tensor(3.1609, grad_fn=<NegBackward0>)\n",
      "17 tensor(3.1594, grad_fn=<NegBackward0>)\n",
      "18 tensor(3.1579, grad_fn=<NegBackward0>)\n",
      "19 tensor(3.1564, grad_fn=<NegBackward0>)\n",
      "20 tensor(3.1550, grad_fn=<NegBackward0>)\n",
      "21 tensor(3.1536, grad_fn=<NegBackward0>)\n",
      "22 tensor(3.1522, grad_fn=<NegBackward0>)\n",
      "23 tensor(3.1508, grad_fn=<NegBackward0>)\n",
      "24 tensor(3.1494, grad_fn=<NegBackward0>)\n",
      "25 tensor(3.1480, grad_fn=<NegBackward0>)\n",
      "26 tensor(3.1467, grad_fn=<NegBackward0>)\n",
      "27 tensor(3.1454, grad_fn=<NegBackward0>)\n",
      "28 tensor(3.1441, grad_fn=<NegBackward0>)\n",
      "29 tensor(3.1427, grad_fn=<NegBackward0>)\n",
      "30 tensor(3.1415, grad_fn=<NegBackward0>)\n",
      "31 tensor(3.1402, grad_fn=<NegBackward0>)\n",
      "32 tensor(3.1389, grad_fn=<NegBackward0>)\n",
      "33 tensor(3.1376, grad_fn=<NegBackward0>)\n",
      "34 tensor(3.1364, grad_fn=<NegBackward0>)\n",
      "35 tensor(3.1352, grad_fn=<NegBackward0>)\n",
      "36 tensor(3.1339, grad_fn=<NegBackward0>)\n",
      "37 tensor(3.1327, grad_fn=<NegBackward0>)\n",
      "38 tensor(3.1315, grad_fn=<NegBackward0>)\n",
      "39 tensor(3.1303, grad_fn=<NegBackward0>)\n",
      "40 tensor(3.1291, grad_fn=<NegBackward0>)\n",
      "41 tensor(3.1279, grad_fn=<NegBackward0>)\n",
      "42 tensor(3.1267, grad_fn=<NegBackward0>)\n",
      "43 tensor(3.1256, grad_fn=<NegBackward0>)\n",
      "44 tensor(3.1244, grad_fn=<NegBackward0>)\n",
      "45 tensor(3.1233, grad_fn=<NegBackward0>)\n",
      "46 tensor(3.1221, grad_fn=<NegBackward0>)\n",
      "47 tensor(3.1210, grad_fn=<NegBackward0>)\n",
      "48 tensor(3.1199, grad_fn=<NegBackward0>)\n",
      "49 tensor(3.1187, grad_fn=<NegBackward0>)\n",
      "50 tensor(3.1176, grad_fn=<NegBackward0>)\n",
      "51 tensor(3.1165, grad_fn=<NegBackward0>)\n",
      "52 tensor(3.1154, grad_fn=<NegBackward0>)\n",
      "53 tensor(3.1143, grad_fn=<NegBackward0>)\n",
      "54 tensor(3.1133, grad_fn=<NegBackward0>)\n",
      "55 tensor(3.1122, grad_fn=<NegBackward0>)\n",
      "56 tensor(3.1111, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m output_layer_activations\n\u001b[1;32m     14\u001b[0m logits_sub_max \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m logits\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 15\u001b[0m counts \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_sub_max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m prob_distributions \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m target_probs \u001b[38;5;241m=\u001b[39m prob_distributions[torch\u001b[38;5;241m.\u001b[39marange(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), y_idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(300):\n",
    "\n",
    "  # forward\n",
    "  embeddings = X @ embedding_matrix\n",
    "  embeddings_flattened = embeddings.view(-1, context_window * embedding_vector_dimensionality) # think about traversal order\n",
    "\n",
    "  hidden_layer_preactivations = embeddings_flattened @ hidden_layer_weights + hidden_layer_biases\n",
    "  hidden_layer_activations = hidden_layer_preactivations.tanh()\n",
    "\n",
    "  output_layer_preactivations = hidden_layer_activations @ output_layer_weights + output_layer_biases\n",
    "  output_layer_activations = output_layer_preactivations\n",
    "  logits = output_layer_activations\n",
    "\n",
    "  logits_sub_max = logits - logits.max(dim=1, keepdim=True).values\n",
    "  counts = logits_sub_max.exp()\n",
    "  prob_distributions = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "  target_probs = prob_distributions[torch.arange(X.shape[0]), y_idx]\n",
    "  target_logprobs = target_probs.log()\n",
    "  negative_average_log_likelihood = -target_logprobs.mean()\n",
    "  loss = negative_average_log_likelihood\n",
    "\n",
    "  losses.append(loss); print(iter, loss)\n",
    "\n",
    "  intermediates = [embeddings, embeddings_flattened, hidden_layer_preactivations, hidden_layer_activations, output_layer_preactivations, output_layer_activations, logits, logits_sub_max, counts, prob_distributions, target_probs, target_logprobs] # new objects created each forward pass, so i think i need to redefine this each time\n",
    "  params_and_intermediates = parameters + intermediates\n",
    "\n",
    "  # backward\n",
    "  for tensor in params_and_intermediates:\n",
    "    tensor.grad = None\n",
    "\n",
    "  learning_rate = 0.01\n",
    "  loss.backward()\n",
    "  embedding_matrix.data = embedding_matrix.data - learning_rate * embedding_matrix.grad\n",
    "  hidden_layer_weights.data = hidden_layer_weights.data - learning_rate * hidden_layer_weights.grad\n",
    "  hidden_layer_biases.data = hidden_layer_biases.data - learning_rate * hidden_layer_biases.grad\n",
    "  output_layer_weights.data = output_layer_weights.data - learning_rate * output_layer_weights.grad\n",
    "  output_layer_biases.data = output_layer_biases.data - learning_rate * output_layer_biases.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
