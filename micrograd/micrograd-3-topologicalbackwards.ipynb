{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, data: float, father: Value = None, mother: Value = None) -> None:\n",
    "    self.data = data\n",
    "    self.father: Value = father\n",
    "    self.mother: Value = mother\n",
    "    self.grad = 0 # gradient w.r.t to the target\n",
    "    self._backward = lambda: None # calculates gradients of parents\n",
    "    self._zero_grad = lambda: None # zeros out gradients of parents\n",
    "\n",
    "\n",
    "  def __repr__(self) -> str:\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  # when backwards called on new node, want to calculate gradients for parents\n",
    "  # you know the derivative of the parent node wrt to the child node\n",
    "  \n",
    "  def __add__(self, other: Value):\n",
    "    father = self; mother = other\n",
    "    child = Value(self.data + other.data, father, mother)\n",
    "    def _backward():\n",
    "      dchild_dfather = 1\n",
    "      dchild_dmother = 1\n",
    "      father.grad += dchild_dfather * child.grad\n",
    "      mother.grad += dchild_dmother * child.grad\n",
    "    def _zero_grad():\n",
    "      father.grad = 0\n",
    "      mother.grad = 0\n",
    "    child._backward = _backward\n",
    "    child._zero_grad = _zero_grad\n",
    "    return child\n",
    "  \n",
    "  def __mul__(self, other: Value):\n",
    "    father = self; mother = other\n",
    "    child = Value(father.data * mother.data, father, mother)\n",
    "    def _backward():\n",
    "      dchild_dfather = mother.data\n",
    "      dchild_dmother = father.data\n",
    "      father.grad += dchild_dfather * child.grad\n",
    "      mother.grad += dchild_dmother * child.grad\n",
    "    def _zero_grad():\n",
    "      father.grad = 0\n",
    "      mother.grad = 0\n",
    "    child._backward = _backward\n",
    "    child._zero_grad = _zero_grad\n",
    "    return child\n",
    "  \n",
    "  def __neg__(self):\n",
    "    father = self\n",
    "    child = Value(-father.data, father)\n",
    "    def _backward():\n",
    "      dchild_dfather = -1\n",
    "      father.grad += dchild_dfather * child.grad\n",
    "    def _zero_grad():\n",
    "      father.grad = 0\n",
    "    child._backward = _backward\n",
    "    child._zero_grad = _zero_grad\n",
    "    return child\n",
    "\n",
    "  def backward(self): # defines this node to be the target. derivative of all nodes in graph wrt to this node. \n",
    "    # edge cases to consider: \n",
    "    # 1. _backward() should only be called when a node's gradient wrt to target is fully computed. i.e. when all it's children have had their _backward's called. EX: a -> b -> c & a -> c. target order is c, b, a but _backward could be called on a before b. b is a's kid. the gradient of a wrt to c is not fully computed.\n",
    "    # 2. _backward should only be called once per node. need some kind of cycle detection. a node could have multiple children, so it could be visited multiple times. just tracking count won't be good enough. need to consider if there's a duplicate visit from the same path. \n",
    "\n",
    "    # could do reverse dfs. root is last to be marked as traversed. if reversing order, it will be called before any of it's children can be called. dfs traverses all children before marking parent as traversed. reversing this means that parent is marked as traversed before any of it's children. this behavior is desirable for topological sorting. from perspective of root, but can be applied to any node b.c. recursive. \n",
    "    # a b c -> c b a. regardless of the path that is taken, b's child, a will always be traversed before b, even if a isn't reached through b. reversing order, a's parents will always be traversed before a.\n",
    "    # adding a traversed variable, so you don't have to check the list. having extra indicator variables save so much computation. \n",
    "\n",
    "    dfs_order = []\n",
    "    def dfs(node: Value):\n",
    "      if node.father:\n",
    "        dfs(node.father)\n",
    "      if node.mother:\n",
    "        dfs(node.mother)\n",
    "      dfs_order.append(node)\n",
    "    dfs(self)\n",
    "\n",
    "    self.grad = 1 # derivative wrt to self is 1\n",
    "    topological_order = dfs_order[::-1]\n",
    "\n",
    "    print(dfs_order)\n",
    "    print(topological_order)\n",
    "\n",
    "    for node in topological_order:\n",
    "      node._backward()\n",
    "\n",
    "  def zero_grad(self): # zeros out gradients of ancestors\n",
    "    def dfs(node: Value):\n",
    "      if node.father:\n",
    "        dfs(node.father)\n",
    "      if node.mother:\n",
    "        dfs(node.mother)\n",
    "      node._zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=2), Value(data=3), Value(data=-6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2)\n",
    "b = Value(3)\n",
    "c = a * -b\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=2), Value(data=3), Value(data=-3), Value(data=-6)]\n",
      "[Value(data=-6), Value(data=-3), Value(data=3), Value(data=2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-3, -2, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad = 1\n",
    "c.backward()\n",
    "a.grad, b.grad, c.grad # only goes one step back. as expected. -b node got calculated, but b node did not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
