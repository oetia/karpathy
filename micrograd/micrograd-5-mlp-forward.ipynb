{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, data: float, father: Value = None, mother: Value = None) -> None:\n",
    "    self.name = \"\"\n",
    "    self.data = data\n",
    "    self.father: Value = father\n",
    "    self.mother: Value = mother\n",
    "    self.grad = 0 # gradient w.r.t to the target\n",
    "    self.traversed = False\n",
    "    self._backward = lambda: None # calculates gradients of parents\n",
    "    self._zero_grad = lambda: None # zeros out gradients of parents\n",
    "\n",
    "\n",
    "  def __repr__(self) -> str:\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  # when backwards called on new node, want to calculate gradients for parents\n",
    "  # you know the derivative of the parent node wrt to the child node\n",
    "  \n",
    "  def __add__(self, other: Value):\n",
    "    father = self; mother = other\n",
    "    child = Value(self.data + other.data, father, mother)\n",
    "    def _backward():\n",
    "      dchild_dfather = 1\n",
    "      dchild_dmother = 1\n",
    "      father.grad += dchild_dfather * child.grad\n",
    "      mother.grad += dchild_dmother * child.grad\n",
    "    def _zero_grad():\n",
    "      father.grad = 0; father.traversed = False\n",
    "      mother.grad = 0; mother.traversed = False\n",
    "    child._backward = _backward\n",
    "    child._zero_grad = _zero_grad\n",
    "    return child\n",
    "  \n",
    "  def __sub__(self, other: Value):\n",
    "    return self + (-other)\n",
    "  \n",
    "  def __mul__(self, other: Value):\n",
    "    father = self; mother = other\n",
    "    child = Value(father.data * mother.data, father, mother)\n",
    "    def _backward():\n",
    "      dchild_dfather = mother.data\n",
    "      dchild_dmother = father.data\n",
    "      father.grad += dchild_dfather * child.grad\n",
    "      mother.grad += dchild_dmother * child.grad\n",
    "    def _zero_grad():\n",
    "      father.grad = 0; father.traversed = False\n",
    "      mother.grad = 0; mother.traversed = False\n",
    "    child._backward = _backward\n",
    "    child._zero_grad = _zero_grad\n",
    "    return child\n",
    "  \n",
    "  def __truediv__(self, other: Value):\n",
    "    return self * (other ** -1)\n",
    "  \n",
    "  def __neg__(self):\n",
    "    father = self\n",
    "    child = Value(-father.data, father)\n",
    "    def _backward():\n",
    "      dchild_dfather = -1\n",
    "      father.grad += dchild_dfather * child.grad\n",
    "    def _zero_grad():\n",
    "      father.grad = 0; father.traversed = False\n",
    "    child._backward = _backward\n",
    "    child._zero_grad = _zero_grad\n",
    "    return child\n",
    "  \n",
    "  def __pow__(self, other: float):\n",
    "    father = self\n",
    "    child = Value(father.data ** other, father)\n",
    "    def _backward():\n",
    "      dchild_dfather = other * father.data ** (other - 1)\n",
    "      father.grad += dchild_dfather * child.grad\n",
    "    def _zero_grad():\n",
    "      father.grad = 0; father.traversed = False\n",
    "    child._backward = _backward\n",
    "    child._zero_grad = _zero_grad\n",
    "    return child\n",
    "\n",
    "\n",
    "  def backward(self): # defines this node to be the target. derivative of all nodes in graph wrt to this node. \n",
    "    # edge cases to consider: \n",
    "    # 1. _backward() should only be called when a node's gradient wrt to target is fully computed. i.e. when all it's children have had their _backward's called. EX: a -> b -> c & a -> c. target order is c, b, a but _backward could be called on a before b. b is a's kid. the gradient of a wrt to c is not fully computed.\n",
    "    # 2. _backward should only be called once per node. need some kind of cycle detection. a node could have multiple children, so it could be visited multiple times. just tracking count won't be good enough. need to consider if there's a duplicate visit from the same path. \n",
    "\n",
    "    # could do reverse dfs. root is last to be marked as traversed. if reversing order, it will be called before any of it's children can be called. dfs traverses all children before marking parent as traversed. reversing this means that parent is marked as traversed before any of it's children. this behavior is desirable for topological sorting. from perspective of root, but can be applied to any node b.c. recursive. \n",
    "    # a b c -> c b a. regardless of the path that is taken, b's child, a will always be traversed before b, even if a isn't reached through b. reversing order, a's parents will always be traversed before a.\n",
    "    # adding a traversed variable, so you don't have to check the list. having extra indicator variables save so much computation. \n",
    "\n",
    "    dfs_order = []\n",
    "    def dfs(node: Value):\n",
    "      if node.father:\n",
    "        dfs(node.father)\n",
    "      if node.mother:\n",
    "        dfs(node.mother)\n",
    "      if not node.traversed: # i forgor traversal check\n",
    "        dfs_order.append(node)\n",
    "        node.traversed = True\n",
    "    dfs(self)\n",
    "\n",
    "    self.grad = 1 # derivative wrt to self is 1\n",
    "    topological_order = dfs_order[::-1]\n",
    "\n",
    "    print(dfs_order)\n",
    "    print(topological_order)\n",
    "\n",
    "    for node in topological_order:\n",
    "      node._backward()\n",
    "\n",
    "  def zero_grad(self): # zeros out gradients of ancestors\n",
    "    self.grad = 0; self.traversed = False\n",
    "    def dfs(node: Value):\n",
    "      if node.father:\n",
    "        dfs(node.father)\n",
    "      if node.mother:\n",
    "        dfs(node.mother)\n",
    "      node._zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=-6), Value(data=36), Value(data=6), Value(data=42)]\n",
      "[Value(data=42), Value(data=6), Value(data=36), Value(data=-6)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-13, 1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(-6)\n",
    "b = a**2 - a\n",
    "b.backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=2), Value(data=3), Value(data=-6), Value(data=36), Value(data=42))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2); a.name = \"a\"\n",
    "b = Value(3); b.name = \"b\"\n",
    "c = a * -b; c.name = \"c\"\n",
    "d = c ** 2; d.name = \"d\"\n",
    "e = d - c; e.name = \"e\"\n",
    "a, b, c, d, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39, 26, -13, 1, 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c.grad = 1 # FUCK ME. THIS WAS THE ISSUE ALL ALONG. Forgot to comment out a line. which then made c gradient 1 higher. i thought the -1 wasn't being applied. it was. \n",
    "e.backward()\n",
    "a.grad, b.grad, c.grad, d.grad, e.grad\n",
    "\n",
    "# e: 1, d: 1, c: (2*-6 * 1 + -1) = -13, b: 26, a: 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all in all, my reasoning was actually on point. the only real mistake i made was forgetting to add traversed var. \n",
    "# the mistake i spent the most time on was just thinking that the -1 wasn't being applied from sub. what happened was that i forgot to uncomment out code that set c.grad to 1, which then meant that the -1 was being applied, but it was 1 + -13 = 12, which effectively canceled it out. i was not expecting the bug to be here and subsequently wasted a ton of time looking elsewhere. \n",
    "# the debugger gave me a massive hint of c.grad being 1 when e's _backward() was called. i was very confused and just was looking for the bug in the wrong place. \n",
    "# the way i ended up fixing this was through just running the error in isolation with new variables and seeing that the result was correct. that led me to know that my backprop calculations were actually correct and that the error was somewhere else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, n_inputs: int):\n",
    "    self.weights = [Value(random.uniform(-1, 1)) for _ in range(n_inputs)]\n",
    "    self.bias = Value(random.uniform(-1, 1))\n",
    "\n",
    "  def __call__(self, inputs: list[Value]):\n",
    "    return sum([weight * input for weight, input in zip(self.weights, inputs)], self.bias)\n",
    "  \n",
    "class Layer:\n",
    "  def __init__(self, n_inputs: int, n_neurons: int):\n",
    "    self.neurons = [Neuron(n_inputs) for _ in range(n_neurons)]\n",
    "  \n",
    "  def __call__(self, inputs: list[Value]):\n",
    "    return [neuron(inputs) for neuron in self.neurons]\n",
    "  \n",
    "\n",
    "class MLP:\n",
    "  def __init__(self, n_inputs: int, layer_neuron_counts: list[int]):\n",
    "    self.layers = [Layer(n_inputs, layer_neuron_counts[0])]\n",
    "    for i in range(1, len(layer_neuron_counts)):\n",
    "      self.layers.append(Layer(layer_neuron_counts[i-1], layer_neuron_counts[i]))\n",
    "\n",
    "  def __call__(self, inputs: list[Value]):\n",
    "    for layer in self.layers:\n",
    "      inputs = layer(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.148619070559517)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP(2, [3, 4, 1])([Value(1), Value(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-1.2124871291593897), Value(data=-3.629437849737884)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Layer(3, 2)([Value(1), Value(2), Value(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Value(data=-0.2943912321182154),\n",
       "  Value(data=0.9571633401668591),\n",
       "  Value(data=0.3592714025282213)],\n",
       " Value(data=0.6075052371454737))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Neuron(3)\n",
    "n.weights, n.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=3.3052548929456407)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n([Value(1), Value(2), Value(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [Value(2.0), Value(3.0), Value(-1.0)],\n",
    "  [Value(3.0), Value(-1.0), Value(0.5)],\n",
    "  [Value(0.5), Value(1.0), Value(1.0)],\n",
    "  [Value(1.0), Value(1.0), Value(-1.0)]\n",
    "]\n",
    "ys = [[Value(1.0)], [Value(-1.0)], [Value(-1.0)], [Value(1.0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3, [4, 4, 1])\n",
    "preds = [mlp(x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=9.540104427677395)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = Value(0)\n",
    "for pred, target in zip(preds, ys):\n",
    "  example_squared_error = Value(0)\n",
    "  for idx in range(len(pred)): # compare each element\n",
    "    example_squared_error += (pred[idx] - target[idx]) ** 2\n",
    "  loss += example_squared_error\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
